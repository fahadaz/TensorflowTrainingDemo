{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "7li6xnbnluzdmgbpuq2n",
   "authorId": "96087873734",
   "authorName": "FAHAD_WEST",
   "authorEmail": "fahad.azeem@snowflake.com",
   "sessionId": "78652b61-4610-44fc-a6ea-736f0b550768",
   "lastEditTime": 1741041086777
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "8a78a58f-a6ea-4086-970e-331857c9f2f8",
   "metadata": {
    "language": "python",
    "name": "install_tensorflow",
    "collapsed": false
   },
   "outputs": [],
   "source": "#!pip install tensorflow #==2.12 keras==2.12\n!pip install tensorflow[and-cuda]==2.14\n!pip install ray[train]\n\n!pip install snowflake-snowpark-python==1.25.0 pandas notebook scikit-learn cachetools pyarrow==10.0.1 python-dotenv",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca95c447-99e4-41d5-9fd8-24f877ce8c21",
   "metadata": {
    "language": "python",
    "name": "check_tf_version",
    "collapsed": false
   },
   "outputs": [],
   "source": "import tensorflow as tf\nprint(tf.__version__)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3ed53c3-875c-4411-843b-c8607c29ed27",
   "metadata": {
    "language": "python",
    "name": "import_libs",
    "collapsed": false
   },
   "outputs": [],
   "source": "import ray\nfrom ray import train\nfrom ray.train.tensorflow import TensorflowTrainer\nfrom ray.train import Checkpoint, ScalingConfig\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom snowflake.ml.data.data_connector import DataConnector\n\n\n# Initialize Snowflake session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3735dac0-d960-41ee-8864-1734843b3bcf",
   "metadata": {
    "language": "sql",
    "name": "setup_permissions",
    "collapsed": false
   },
   "outputs": [],
   "source": "use role accountadmin;\ncreate schema if not exists tensorflow_data.feature_store;\ngrant usage on database tensorflow_data to sysadmin;\ngrant usage on schema tensorflow_data.public to sysadmin;\ngrant usage on schema tensorflow_data.feature_store to sysadmin;\ngrant create dynamic table, create tag, create stage, create view on schema tensorflow_data.feature_store to sysadmin;\ngrant select on table tensorflow_data.public.PENGUINS to sysadmin;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "936aa8cd-b6c5-4752-9cea-448efecf3469",
   "metadata": {
    "language": "sql",
    "name": "cell10",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE SEQUENCE If NOT EXISTS tensorflow_data.public.my_table_seq START = 1 INCREMENT = 1;\nALTER TABLE tensorflow_data.public.PENGUINS ADD COLUMN id INT;\nUPDATE tensorflow_data.public.PENGUINS SET id = my_table_seq.nextval;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afe477ed-2b31-42bf-acd3-ba37af453b48",
   "metadata": {
    "language": "python",
    "name": "create_feature_store",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import FeatureStore, CreationMode, Entity, FeatureView\n\nfs = FeatureStore(\n    session=session,\n    database='tensorflow_data',\n    name='feature_store',\n    default_warehouse='QUICKSTART_WH',\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44a2ee08-1411-4bf7-a548-884c2893cd98",
   "metadata": {
    "language": "python",
    "name": "create_entities",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "entity = Entity(\n    name=\"PENGUIN_ID\",\n    join_keys=[\"id\"],\n    desc=\"Penguin ID\"\n)\n\nfs.register_entity(entity)\n\n# Show our newly created entity\n# snowpark.DataFrame.show() is another way to preview the DataFrame contents\nfs.list_entities().show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64fcb0bf-197e-4fc0-a9fc-b5dc98dca0e0",
   "metadata": {
    "language": "sql",
    "name": "cell4",
    "collapsed": false
   },
   "outputs": [],
   "source": "select distinct SPECIES from TENSORFLOW_DATA.PUBLIC.PENGUINS --where SEX in ('male', 'female');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99dba8ec-c5c0-44e0-a2b1-95c6090b4e71",
   "metadata": {
    "language": "sql",
    "name": "create_stage_location_for_udf",
    "collapsed": false
   },
   "outputs": [],
   "source": "create or replace stage TENSORFLOW_DATA.FEATURE_STORE.UDF_STAGE\nDIRECTORY = ( ENABLE = TRUE )\nCOMMENT = 'Stage for UDF functions'; ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f11f79a-cafd-4576-95aa-07f40fb121bd",
   "metadata": {
    "language": "python",
    "name": "Create_udf_to_use",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import udf\nfrom snowflake.snowpark.types import IntegerType\n\n@udf(return_type=IntegerType(),name=\"penguin_species_to_int\", is_permanent=True, replace=True, stage_location=\"@TENSORFLOW_DATA.FEATURE_STORE.UDF_STAGE\")  # Specify the return data type\ndef penguin_species_to_int(species: str):\n    if species == \"Adelie\":\n        return 0\n    elif species == \"Gentoo\":\n        return 1\n    elif species == \"Chinstrap\":\n        return 2\n    else:  # Handle other species or NULLs appropriately\n        return -1  # Or raise an exception, or return NULL, depending on your needs",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60627c25-22ab-401a-815f-1c3e7dd122d1",
   "metadata": {
    "language": "python",
    "name": "create_df_for_featureview",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\n\ntable_name = 'TENSORFLOW_DATA.PUBLIC.PENGUINS'\nsnowpark_df = session.table(table_name).filter(F.col(\"SEX\").in_('male','female'))\n#snowpark_df = session.table(table_name).dropna(subset=['BILL_LENGTH_MM', 'BILL_DEPTH_MM','FLIPPER_LENGTH_MM','BODY_MASS_G'])\n\nfeature_df = snowpark_df.select(                    \n                   F.col(\"ID\"), \n                   F.col(\"BILL_LENGTH_MM\").cast(\"float\").alias(\"BILL_LENGTH_MM\"),\n                   F.col(\"BILL_DEPTH_MM\").cast(\"float\").alias(\"BILL_DEPTH_MM\"),\n                   (F.col(\"FLIPPER_LENGTH_MM\").cast(\"float\")/10).alias(\"FLIPPER_LENGTH_MM\"), #normalize\n                   (F.col(\"BODY_MASS_G\").cast(\"float\")/100).alias(\"BODY_MASS_G\"), #normalize\n                    F.col(\"SPECIES\")\n                   #F.call_udf(\"TENSORFLOW_DATA.FEATURE_STORE.PENGUIN_SPECIES_TO_INT\",F.col(\"SPECIES\")).alias(\"SPECIES_INT\"),                   \n                  )\n\nfeature_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8eb5df03-8670-4603-a710-3ded402ffbba",
   "metadata": {
    "language": "python",
    "name": "workinprogress",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import snowflake.snowpark.functions as F\nfrom snowflake.snowpark.window import Window\n\n\n# Create a window partitioned by 'ID'\nwindow_spec = Window.partition_by(\"ID\").order_by(\"SPECIES\")\n\n# Calculate the standard deviation using the window function\nresult_df = feature_df.with_column(\n    \"STDDEV_BODY_MASS_G\",\n    F.stddev(F.col(\"BODY_MASS_G\").cast(\"float\")).over(window_spec)\n)\n\n# Show the results (optional)\nresult_df.show()\n\n# If you only want the id and the standard deviation, you can select only those columns.\nfinal_result_df = result_df.select(\"ID\", \"STDDEV_BODY_MASS_G\")\nfinal_result_df.show()\n\n# If you want the population standard deviation, use stddev_pop instead of stddev\nresult_pop_df = feature_df.with_column(\n    \"STDDEV_POP_BODY_MASS_G\",\n    F.stddev_pop(F.col(\"BODY_MASS_G\").cast(\"float\")).over(window_spec)\n)\n\nresult_pop_df.show()\n\nfinal_pop_result_df = result_pop_df.select(\"ID\", \"STDDEV_POP_BODY_MASS_G\")\nfinal_pop_result_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30589bab-9f0a-4736-808d-612699d6207f",
   "metadata": {
    "language": "python",
    "name": "register_feature_view",
    "collapsed": false
   },
   "outputs": [],
   "source": "pen_fv = FeatureView(\n    name=\"Penguin_Data\",\n    entities = [entity],\n    feature_df = feature_df    \n)\n\npen_nn_fv = fs.register_feature_view(pen_fv, version=\"1\", overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d4ea367-d6db-45e4-b0d9-250207f09c0d",
   "metadata": {
    "language": "python",
    "name": "generate_dataset_for_training_testing",
    "collapsed": false
   },
   "outputs": [],
   "source": "spine_df = session.table(\"TENSORFLOW_DATA.PUBLIC.PENGUINS\")\nspine_df = spine_df.filter(F.col(\"SEX\").in_('male','female')).select(F.col(\"ID\"))\n\ntraining_dataset = fs.generate_dataset(\n    name=\"PENGUIN_TRAINING_DATASET\",\n    spine_df = spine_df,\n    features=[pen_nn_fv],\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13dc16e1-a41b-482e-9f58-7b121523019a",
   "metadata": {
    "language": "python",
    "name": "load_data_using_dataconnector",
    "collapsed": false
   },
   "outputs": [],
   "source": "#Loading training Data Set\npenguins = DataConnector.from_dataset(training_dataset).to_pandas()\n\n# Convert to Ray Dataframe\npenguins_ray_ds = ray.data.from_pandas(penguins)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "128f6fa7-d388-4a0a-8d1c-0c119f03b4cb",
   "metadata": {
    "language": "python",
    "name": "feature_and_one_hot_encoding",
    "collapsed": false
   },
   "outputs": [],
   "source": "features = [\"BILL_LENGTH_MM\", \"BILL_DEPTH_MM\", \"FLIPPER_LENGTH_MM\" , \"BODY_MASS_G\"]\nlabel = 'SPECIES'\n\ndef one_hot_encode(row):\n    row['features'] = [float(row['BILL_LENGTH_MM']) , float(row['BILL_DEPTH_MM']), float(row['FLIPPER_LENGTH_MM'])\n                       , float(row['BODY_MASS_G'])]\n    \n    match row['SPECIES']:\n        case 'Adelie':\n            row['label'] = [1, 0, 0]\n        case 'Gentoo':\n            row['label'] = [0, 1, 0]\n        case 'Chinstrap':\n            row['label'] = [0, 0, 1]\n    return row\n\n#selecting only features and label arrays\npenguins_ray_ds = penguins_ray_ds.map(one_hot_encode).select_columns(['features','label'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5b03e31-a783-4ba2-9c18-62a375e32597",
   "metadata": {
    "language": "python",
    "name": "get_gpu_count",
    "collapsed": false
   },
   "outputs": [],
   "source": "known_to_ray_gpus = ray.cluster_resources()\n\ngpu_count = int(known_to_ray_gpus.get(\"GPU\",0))\n\nprint(\"Total GPUS known to Ray:\", gpu_count)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bac3c44c-51c1-47d4-ace6-bdc8bb0ebc7b",
   "metadata": {
    "language": "python",
    "name": "test_train_split_with_ray",
    "collapsed": false
   },
   "outputs": [],
   "source": "import ray\nimport pandas as pd\nimport numpy as np\n\n\n# Split into train test split\ntrain_rs, test_rs = penguins_ray_ds.train_test_split(test_size=0.2, shuffle=True, seed=42)  # Split into 2 datasets",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef6442fc-8b3a-4177-853f-9cd1eaa4b96b",
   "metadata": {
    "language": "python",
    "name": "import_tensorflow_keras_libs",
    "collapsed": false
   },
   "outputs": [],
   "source": "import tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Set random seed for reproducability\ntensorflow.random.set_seed(0)\n\nprint(\"Libraries imported.\")\nprint('TensorFlow version:',tensorflow.__version__)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f8aa1dd-89ca-42c0-a156-f5a4f14b6198",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "from matplotlib import pyplot as plt\n\nepoch_nums = range(1,num_epochs+1)\ntraining_loss = history.history[\"loss\"]\nvalidation_loss = history.history[\"val_loss\"]\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()"
  },
  {
   "cell_type": "code",
   "id": "6040f411-2c9e-492c-adf6-bc70bd25face",
   "metadata": {
    "language": "python",
    "name": "distributed_training",
    "collapsed": false
   },
   "outputs": [],
   "source": "import os\nimport tempfile, json\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\n\nfrom ray import train\nfrom ray.train import Checkpoint, ScalingConfig\nfrom ray.train.tensorflow import TensorflowTrainer\n\npenguin_classes = ['Adelie', 'Gentoo', 'Chinstrap']\nfeatures = [\"BILL_LENGTH_MM\", \"BILL_DEPTH_MM\", \"FLIPPER_LENGTH_MM\" , \"BODY_MASS_G\"]\nlabel = 'SPECIES_INT'\n\ndef build_model():\n    # Define a classifier network\n    hl = 10 # Number of hidden layer nodes\n\n    model = Sequential()\n    model.add(Dense(hl, input_dim=len(features), activation='relu'))\n    model.add(Dense(hl, input_dim=hl, activation='relu'))\n    model.add(Dense(len(penguin_classes), input_dim=hl, activation='softmax'))    \n    return model\n\ndef train_loop_per_worker(config):\n    print(config)\n    dataset_shard = train.get_dataset_shard(\"train\")\n          \n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    \n    with strategy.scope():\n        model = build_model()\n        #hyper-parameters for optimizer\n        learning_rate = 0.001\n        opt = optimizers.Adam(lr=learning_rate)\n        \n        model.compile(             \n            loss=\"categorical_crossentropy\", \n            optimizer=opt, metrics=[\"accuracy\"]\n        )\n        \n        print(model.summary())\n\n    tf_dataset = dataset_shard.to_tf(        \n        feature_columns='features',\n        label_columns='label',\n        batch_size=10\n    )\n        \n    for epoch in range(config[\"num_epochs\"]):\n        history = model.fit(tf_dataset)\n\n        #saving model for later loading\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            model.save(os.path.join(temp_checkpoint_dir, \"model.keras\"))\n            checkpoint_dict = os.path.join(temp_checkpoint_dir, \"checkpoint.json\")\n            with open(checkpoint_dict, \"w\") as f:\n                json.dump({\"epoch\": epoch}, f)\n            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n\n            train.report({\"loss\": history.history[\"loss\"][0], \"accuracy\": history.history[\"accuracy\"][0]}, checkpoint=checkpoint)\n\ntrainer = TensorflowTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    scaling_config=ScalingConfig(num_workers= gpu_count, use_gpu=True),\n    datasets={\"train\": train_rs},\n    train_loop_config={\"num_epochs\": 50},\n)\n\nresult = trainer.fit()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61c22bda-69d5-46ef-af21-441063d3e844",
   "metadata": {
    "language": "python",
    "name": "load_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "trained_model = tf.keras.models.load_model(result.checkpoint.path+'/model.keras')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4f94430-8576-4034-8944-aeb7f083e6ef",
   "metadata": {
    "language": "python",
    "name": "inference_test_dataset",
    "collapsed": false
   },
   "outputs": [],
   "source": "sample_data = test_rs.to_tf(feature_columns=\"features\", label_columns=\"label\")\n\n#print(test_rs.to_pandas())\n\nres = trained_model.predict(x=sample_data)\n\nnp.set_printoptions(suppress=True)\n#print(res)\n\ndataset = tf.data.Dataset.from_tensor_slices(res)\n\ndef find_max_index(array):\n    return tf.argmax(array)  # No need to specify axis for a 1D array\n    \n# Apply the map function\ndataset = dataset.map(find_max_index)\n\n# Iterate and print the results\nprint(penguin_classes)\nfor index in dataset:\n    print(penguin_classes[index.numpy()])  ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe24bd4c-ae05-44f4-b159-2c31495ad372",
   "metadata": {
    "language": "python",
    "name": "get_model_registry",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\nreg = Registry(session=session, database_name=\"TENSORFLOW_DATA\", schema_name=\"FEATURE_STORE\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6af93bc7-0b2b-465c-a1a8-76f6fb812761",
   "metadata": {
    "language": "python",
    "name": "log_model_to_registry",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\ninput = {'features':[[1.,2.,3.,4.]]}\noutput = {'label':[[1,0,0]]}\n\ninput_df = pd.DataFrame(input)\noutput_df = pd.DataFrame(output)\n\nfrom snowflake.ml.model import model_signature\n\n#df = test_rs.to_pandas()\n\n\npredict_signature = model_signature.infer_signature(input_data=input_df, output_data=output_df)\n\nprint(predict_signature)\n\nmv = reg.log_model(\n    trained_model, \n    model_name ='tf_pengiun_model', \n    version_name='v1', \n    pip_requirements=[\"tensorflow[and-cuda]==2.14\"],\n    signatures={\"predict\": predict_signature},\n    #sample_input_data= df,\n    target_platforms=['SNOWPARK_CONTAINER_SERVICES']\n)\n ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15621a4c-a7c2-4102-a32e-b95cb5741d2d",
   "metadata": {
    "language": "python",
    "name": "cell7_sample",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import os\nimport tempfile\nimport tensorflow as tf\n\nimport ray\nfrom ray import train\nfrom ray.train import Checkpoint, ScalingConfig\nfrom ray.train.tensorflow import TensorflowTrainer\n\ndef build_model():\n    # toy neural network : 1-layer\n    return tf.keras.Sequential(\n        [tf.keras.layers.Dense(\n            1, activation=\"linear\", input_shape=(1,))]\n    )\n\ndef train_loop_per_worker(config):\n    dataset_shard = train.get_dataset_shard(\"train\")\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    with strategy.scope():\n        model = build_model()\n        model.compile(\n            optimizer=\"Adam\", loss=\"mean_squared_error\", metrics=[\"mse\"])\n\n    tf_dataset = dataset_shard.to_tf(\n        feature_columns=\"x\",\n        label_columns=\"y\",\n        batch_size=1\n    )\n    for epoch in range(config[\"num_epochs\"]):\n        model.fit(tf_dataset)\n\n        # Create checkpoint.\n        checkpoint_dir = tempfile.mkdtemp()\n        model.save_weights(\n            os.path.join(checkpoint_dir, \"my_checkpoint\")\n        )\n        checkpoint = Checkpoint.from_directory(checkpoint_dir)\n\n        train.report(\n            {},\n            checkpoint=checkpoint,\n        )\n\ntrain_dataset = ray.data.from_items([{\"x\": x, \"y\": x + 1} for x in range(32)])\n\nprint(train_dataset)\n\ntrainer = TensorflowTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    scaling_config=ScalingConfig(num_workers=3, use_gpu=True),\n    datasets={\"train\": train_dataset},\n    train_loop_config={\"num_epochs\": 4},\n)\n#result = trainer.fit()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "040fb929-9b90-4558-a6ae-64b76517cbd6",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "SELECT CURRENT_ACCOUNT_LOCATOR();",
   "execution_count": null
  }
 ]
}