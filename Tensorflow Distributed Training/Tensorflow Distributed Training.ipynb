{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "7li6xnbnluzdmgbpuq2n",
   "authorId": "96087873734",
   "authorName": "FAHAD_WEST",
   "authorEmail": "fahad.azeem@snowflake.com",
   "sessionId": "80197960-be82-41f5-9588-40cf84179b31",
   "lastEditTime": 1741715945842
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "8a78a58f-a6ea-4086-970e-331857c9f2f8",
   "metadata": {
    "language": "python",
    "name": "install_tensorflow",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "%pip install tensorflow[and-cuda]==2.14\n%pip install ray[train]\n\n%pip install snowflake-snowpark-python==1.25.0 snowflake-ml-python==1.7.5 snowflake==1.0.5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ca95c447-99e4-41d5-9fd8-24f877ce8c21",
   "metadata": {
    "language": "python",
    "name": "check_tf_version",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import tensorflow as tf\nprint(tf.__version__)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3735dac0-d960-41ee-8864-1734843b3bcf",
   "metadata": {
    "language": "sql",
    "name": "setup_permissions",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "use role accountadmin;\ncreate schema if not exists tensorflow_data.feature_store;\ngrant usage on database tensorflow_data to sysadmin;\ngrant usage on schema tensorflow_data.public to sysadmin;\ngrant usage on schema tensorflow_data.feature_store to sysadmin;\ngrant create dynamic table, create tag, create stage, create view on schema tensorflow_data.feature_store to sysadmin;\ngrant select on table tensorflow_data.public.PENGUINS to sysadmin;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57bcd2b8-b1a9-40b8-ae74-8b7e58b414a7",
   "metadata": {
    "name": "chinstrap_gentoo_adelie_graphic",
    "collapsed": false
   },
   "source": "![](https://www.tensorflow.org/tutorials/customization/images/penguins_ds_species.png)"
  },
  {
   "cell_type": "code",
   "id": "7673438c-be99-4d93-b33e-317baa69fecb",
   "metadata": {
    "language": "sql",
    "name": "define_table"
   },
   "outputs": [],
   "source": "use database TENSORFLOW_DATA;\nuse schema public;\n\n-- Define Sequence\nCREATE or replace SEQUENCE penguin_seq START = 1 INCREMENT = 1;\n\ncreate or replace table PENGUINS(\n    ID NUMBER DEFAULT penguin_seq.NEXTVAL,\n    SPECIES VARCHAR(20),\n\tISLAND VARCHAR(20),\n\tBILL_LENGTH_MM NUMBER(10,2),\n\tBILL_DEPTH_MM NUMBER(10,2),\n\tFLIPPER_LENGTH_MM NUMBER(10,2),\n\tBODY_MASS_G NUMBER(10,2),\n\tSEX VARCHAR(20),\n\tYEAR NUMBER(4,0)\t\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9863f90-8892-4afe-ac0d-9bf11aa75f75",
   "metadata": {
    "language": "sql",
    "name": "import_data"
   },
   "outputs": [],
   "source": "COPY INTO \"TENSORFLOW_DATA\".\"PUBLIC\".\"PENGUINS\"\nFROM (\n    SELECT penguin_seq.NEXTVAL, $1, $2, $3, $4, $5, $6, $7, $8\n    FROM '@\"TENSORFLOW_DATA\".\"PUBLIC\".\"RAW_DATA\"'\n)\nFILES = ('penguins.csv')\nFILE_FORMAT = (\n    TYPE=CSV,\n    NULL_IF = ('NA', 'N/A', 'na', 'n/a')\n    SKIP_HEADER=1,\n    FIELD_DELIMITER=',',\n    TRIM_SPACE=TRUE,\n    FIELD_OPTIONALLY_ENCLOSED_BY='\"',\n    REPLACE_INVALID_CHARACTERS=TRUE,\n    DATE_FORMAT=AUTO,\n    TIME_FORMAT=AUTO,\n    TIMESTAMP_FORMAT=AUTO\n)\nON_ERROR=ABORT_STATEMENT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e9c9e50-dc05-4ee2-a768-0439ee41bc96",
   "metadata": {
    "language": "sql",
    "name": "view_imported_data"
   },
   "outputs": [],
   "source": "select * from TENSORFLOW_DATA.PUBLIC.PENGUINS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d45aa1b-e837-4b3d-b929-adb9decf056e",
   "metadata": {
    "language": "sql",
    "name": "setup_event_table"
   },
   "outputs": [],
   "source": "-- Define event table\nCREATE EVENT TABLE IF NOT EXISTS tensorflow_data.public.events;\n-- Set event table for the database\nALTER DATABASE tensorflow_data SET EVENT_TABLE = tensorflow_data.public.events;\n-- Set database logging level\nALTER DATABASE tensorflow_data SET LOG_LEVEL = INFO;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0128bd17-42ce-4d12-854c-2a77466cf900",
   "metadata": {
    "language": "python",
    "name": "import_libs_create_session"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom snowflake.ml.data.data_connector import DataConnector\n\n# Initialize Snowflake session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afe477ed-2b31-42bf-acd3-ba37af453b48",
   "metadata": {
    "language": "python",
    "name": "create_feature_store",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import FeatureStore, CreationMode, Entity, FeatureView\n\nfs = FeatureStore(\n    session=session,\n    database='tensorflow_data',\n    name='feature_store',\n    default_warehouse='QUICKSTART_WH',\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44a2ee08-1411-4bf7-a548-884c2893cd98",
   "metadata": {
    "language": "python",
    "name": "create_entities",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "entity = Entity(\n    name=\"PENGUIN_ID\",\n    join_keys=[\"id\"],\n    desc=\"Penguin ID\"\n)\n\nfs.register_entity(entity)\n\n# Show our newly created entity\nfs.list_entities().show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99dba8ec-c5c0-44e0-a2b1-95c6090b4e71",
   "metadata": {
    "language": "sql",
    "name": "create_stage_location_for_udf",
    "collapsed": false
   },
   "outputs": [],
   "source": "create or replace stage TENSORFLOW_DATA.FEATURE_STORE.UDF_STAGE\nDIRECTORY = ( ENABLE = TRUE )\nCOMMENT = 'Stage for UDF functions'; ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f11f79a-cafd-4576-95aa-07f40fb121bd",
   "metadata": {
    "language": "python",
    "name": "Create_udf_to_use",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import udf\nfrom snowflake.snowpark.types import IntegerType\n\n@udf(return_type=IntegerType(),name=\"penguin_species_to_int\", is_permanent=True, replace=True, stage_location=\"@TENSORFLOW_DATA.FEATURE_STORE.UDF_STAGE\")  # Specify the return data type\ndef penguin_species_to_int(species: str):\n    if species == \"Adelie\":\n        return 0\n    elif species == \"Gentoo\":\n        return 1\n    elif species == \"Chinstrap\":\n        return 2\n    else:  # Handle other species or NULLs appropriately\n        return -1  # Or raise an exception, or return NULL, depending on your needs",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ab056bf-9052-469d-bb4c-663f235e24b2",
   "metadata": {
    "language": "python",
    "name": "vudtf_example"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import udtf\nfrom snowflake.snowpark.types import IntegerType\n\nfrom snowflake.snowpark.types import StructType, StructField, FloatType, StringType\nfrom snowflake.snowpark.functions import col, udtf\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nclass BodyConditionPCA:\n    \n    def standardize_features(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\" Standardizes the numeric feature columns. \"\"\"\n        scaler = StandardScaler()\n        return scaler.fit_transform(df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]])\n\n    @vectorized(input=pd.DataFrame)\n    def end_partition(self, species: str, sex: str, df: pd.DataFrame):\n        \"\"\" Apply PCA on standardized features and return the Body Condition Score for each row in the partition. \"\"\"\n\n        # Standardize feature columns\n        standardized_features = self.standardize_features(df)\n\n        # Perform PCA (first principal component)\n        pca = PCA(n_components=1)\n        body_condition_scores = pca.fit_transform(standardized_features).flatten()\n\n        # Add Body Condition Score to DataFrame\n        df[\"body_condition_score\"] = body_condition_scores\n\n        # Return as tuples (ensuring compatibility with Snowflake output format)\n        return df\n\n# Register the UDTF in Snowflake\n# session = Session.builder.getOrCreate()\n\nsession.udtf.register(\n    BodyConditionPCA,\n    output_schema=StructType([\n        StructField(\"species\", StringType()),\n        StructField(\"sex\", StringType()),\n        StructField(\"bill_length_mm\", FloatType()),\n        StructField(\"bill_depth_mm\", FloatType()),\n        StructField(\"flipper_length_mm\", FloatType()),\n        StructField(\"body_mass_g\", FloatType()),\n        StructField(\"body_condition_score\", FloatType()),\n    ]),\n    input_types=[StringType(), StringType(), FloatType(), FloatType(), FloatType(), FloatType()],\n    name=\"BodyConditionPCA\",\n    replace=True,\n    is_permanent=False,\n    packages=['pandas', 'numpy', 'scikit-learn']\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60627c25-22ab-401a-815f-1c3e7dd122d1",
   "metadata": {
    "language": "python",
    "name": "create_df_for_featureview",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\n\ntable_name = 'TENSORFLOW_DATA.PUBLIC.PENGUINS'\n\n#Droping rows with NULL values\nsnowpark_df = session.table(table_name).dropna()\n\nfeature_df = snowpark_df.select(                    \n                   F.col(\"ID\"), \n                   F.col(\"BILL_LENGTH_MM\").cast(\"float\").alias(\"BILL_LENGTH_MM\"),\n                   F.col(\"BILL_DEPTH_MM\").cast(\"float\").alias(\"BILL_DEPTH_MM\"),\n                   (F.col(\"FLIPPER_LENGTH_MM\").cast(\"float\")/10).alias(\"FLIPPER_LENGTH_MM\"), #normalize\n                   (F.col(\"BODY_MASS_G\").cast(\"float\")/100).alias(\"BODY_MASS_G\"), #normalize\n                    F.col(\"SPECIES\")                   \n                  )\n\nfeature_df.show(n=5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30589bab-9f0a-4736-808d-612699d6207f",
   "metadata": {
    "language": "python",
    "name": "register_feature_view",
    "collapsed": false
   },
   "outputs": [],
   "source": "pen_fv = FeatureView(\n    name=\"Penguin_Data\",\n    entities = [entity],\n    feature_df = feature_df,\n    refresh_freq= '5 minutes',\n    desc=\"Penguin Data managed feature view\"\n)\n\npen_nn_fv = fs.register_feature_view(pen_fv, version=\"1\", overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "228aa66d-204e-4c38-9d09-0187097d55d1",
   "metadata": {
    "language": "python",
    "name": "get_ddl_for_approval"
   },
   "outputs": [],
   "source": "import streamlit as st\n# For Views\n#session.sql(f\"\"\"SELECT GET_DDL('DYNAMIC_TABLE', '{pen_nn_fv.fully_qualified_name()}');\"\"\").collect()[0][0]\n\n# For Dynamic Tables \nstr_sql = session.sql(f\"\"\"SELECT GET_DDL('DYNAMIC_TABLE', '{pen_nn_fv.fully_qualified_name()}');\"\"\").collect()[0][0]\n\nst.text(str_sql)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d4ea367-d6db-45e4-b0d9-250207f09c0d",
   "metadata": {
    "language": "python",
    "name": "generate_dataset_for_training_testing",
    "collapsed": false
   },
   "outputs": [],
   "source": "#Datasets are new Snowflake schema-level objects specially designed for machine learning workflows. \n#Snowflake Datasets hold collections of data organized into versions, \n#where each version holds a materialized snapshot of your data with guaranteed immutability, \n#efficient data access, and interoperability with popular deep learning frameworks.\n\nspine_df = session.table(\"TENSORFLOW_DATA.PUBLIC.PENGUINS\").dropna().select(F.col(\"ID\"))\n\ntraining_dataset = fs.generate_dataset(\n    name=\"PENGUIN_TRAINING_DATASET\",\n    spine_df = spine_df,\n    features=[pen_nn_fv],\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ef0fe03-0c06-40c4-b88f-dc95bfae706b",
   "metadata": {
    "language": "sql",
    "name": "show_dataset_in_db"
   },
   "outputs": [],
   "source": "show datasets in database;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c796307-73dc-4a38-86e3-0950925b6bb5",
   "metadata": {
    "language": "sql",
    "name": "show_events"
   },
   "outputs": [],
   "source": "select * from events order by timestamp desc;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "276ffa58-908d-4a5a-ac85-bfe06f9e6cae",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "**Model Training**"
  },
  {
   "cell_type": "code",
   "id": "fb5441fd-24d5-4eaf-9fe7-841d33934fbc",
   "metadata": {
    "language": "python",
    "name": "import_ray_library"
   },
   "outputs": [],
   "source": "import ray\nfrom ray import train\nfrom ray.train.tensorflow import TensorflowTrainer\nfrom ray.train import Checkpoint, ScalingConfig\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Make Ray output less verbose\ncontext = ray.data.DataContext().get_current() \ncontext.execution_options.verbose_progress = False\ncontext.enable_operator_progress_bars = False\ncontext.enable_progress_bars = False",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13dc16e1-a41b-482e-9f58-7b121523019a",
   "metadata": {
    "language": "python",
    "name": "load_data_using_dataconnector",
    "collapsed": false
   },
   "outputs": [],
   "source": "#Loading training Data Set\ndata_connector = DataConnector.from_dataset(training_dataset)\n\n# Getting Ray Data Set\npenguins_ray_ds = data_connector._ingestor.ray_ds",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "128f6fa7-d388-4a0a-8d1c-0c119f03b4cb",
   "metadata": {
    "language": "python",
    "name": "feature_and_one_hot_encoding",
    "collapsed": false
   },
   "outputs": [],
   "source": "features = [\"BILL_LENGTH_MM\", \"BILL_DEPTH_MM\", \"FLIPPER_LENGTH_MM\" , \"BODY_MASS_G\"]\nlabel = 'SPECIES'\n\ndef one_hot_encode_format_input(row):\n    row['features'] = [float(row['BILL_LENGTH_MM']) , float(row['BILL_DEPTH_MM']), float(row['FLIPPER_LENGTH_MM'])\n                       , float(row['BODY_MASS_G'])]\n    \n    match row['SPECIES']:\n        case 'Adelie':\n            row['label'] = [1, 0, 0]\n        case 'Gentoo':\n            row['label'] = [0, 1, 0]\n        case 'Chinstrap':\n            row['label'] = [0, 0, 1]\n    return row\n\n#selecting only features and label arrays\npenguins_ray_ds = penguins_ray_ds.map(one_hot_encode_format_input).select_columns(['features','label'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5b03e31-a783-4ba2-9c18-62a375e32597",
   "metadata": {
    "language": "python",
    "name": "get_gpu_count",
    "collapsed": false
   },
   "outputs": [],
   "source": "known_to_ray_gpus = ray.cluster_resources()\n\ngpu_count = int(known_to_ray_gpus.get(\"GPU\",0))\n\nprint(\"Total GPUS known to Ray:\", gpu_count)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bac3c44c-51c1-47d4-ace6-bdc8bb0ebc7b",
   "metadata": {
    "language": "python",
    "name": "test_train_split_with_ray",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Split into train test split\ntrain_rs, test_rs = penguins_ray_ds.train_test_split(test_size=0.2, shuffle=True, seed=42)  # Split into 2 datasets",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef6442fc-8b3a-4177-853f-9cd1eaa4b96b",
   "metadata": {
    "language": "python",
    "name": "import_tensorflow_keras_libs",
    "collapsed": false
   },
   "outputs": [],
   "source": "import tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Set random seed for reproducability\ntensorflow.random.set_seed(0)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29c1be46-54d1-4b3f-a063-2bea80faba1b",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "![](https://www.tensorflow.org/tutorials/customization/images/full_network_penguin.png)"
  },
  {
   "cell_type": "code",
   "id": "6040f411-2c9e-492c-adf6-bc70bd25face",
   "metadata": {
    "language": "python",
    "name": "distributed_training",
    "collapsed": false
   },
   "outputs": [],
   "source": "import os\nimport tempfile, json\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\n\nfrom ray import train\nfrom ray.train import Checkpoint, ScalingConfig\nfrom ray.train.tensorflow import TensorflowTrainer\n\npenguin_classes = ['Adelie', 'Gentoo', 'Chinstrap']\nfeatures = [\"BILL_LENGTH_MM\", \"BILL_DEPTH_MM\", \"FLIPPER_LENGTH_MM\" , \"BODY_MASS_G\"]\nlabel = 'SPECIES_INT'\n\ndef build_model():\n    # Define a classifier network\n    hl = 10 # Number of hidden layer nodes\n\n    model = Sequential()\n    model.add(Dense(hl, input_dim=len(features), activation='relu'))\n    model.add(Dense(hl, input_dim=hl, activation='relu'))\n    model.add(Dense(len(penguin_classes), input_dim=hl, activation='softmax'))    \n    return model\n\ndef train_loop_per_worker(config):\n    print(config)\n    dataset_shard = train.get_dataset_shard(\"train\")\n          \n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n    \n    with strategy.scope():\n        model = build_model()\n        #hyper-parameters for optimizer\n        learning_rate = 0.001\n        opt = optimizers.Adam(lr=learning_rate)\n        \n        model.compile(             \n            loss=\"categorical_crossentropy\", \n            optimizer=opt, metrics=[\"accuracy\"]\n        )\n        \n        print(model.summary())\n\n    tf_dataset = dataset_shard.to_tf(        \n        feature_columns='features',\n        label_columns='label',\n        batch_size=10\n    )\n        \n    for epoch in range(config[\"num_epochs\"]):\n        history = model.fit(tf_dataset)\n\n        #saving model for later loading\n        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n            model.save(os.path.join(temp_checkpoint_dir, \"model.keras\"))\n            checkpoint_dict = os.path.join(temp_checkpoint_dir, \"checkpoint.json\")\n            with open(checkpoint_dict, \"w\") as f:\n                json.dump({\"epoch\": epoch}, f)\n            checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n\n            train.report({\"loss\": history.history[\"loss\"][0], \"accuracy\": history.history[\"accuracy\"][0]}, checkpoint=checkpoint)\n\ntrainer = TensorflowTrainer(\n    train_loop_per_worker=train_loop_per_worker,\n    scaling_config=ScalingConfig(num_workers= gpu_count, use_gpu=True),\n    datasets={\"train\": train_rs},\n    train_loop_config={\"num_epochs\": 50},\n)\n\nresult = trainer.fit()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61c22bda-69d5-46ef-af21-441063d3e844",
   "metadata": {
    "language": "python",
    "name": "load_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "trained_model = tf.keras.models.load_model(result.checkpoint.path+'/model.keras')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4f94430-8576-4034-8944-aeb7f083e6ef",
   "metadata": {
    "language": "python",
    "name": "inference_test_dataset",
    "collapsed": false
   },
   "outputs": [],
   "source": "test_data = test_rs.to_tf(feature_columns=\"features\", label_columns=\"label\")\n\nres = trained_model.predict(x=test_data)\n\nnp.set_printoptions(suppress=True)\n\n# Get Inference for Test dataset\ninference_dataset = tf.data.Dataset.from_tensor_slices(res)\n\n# Combine Inference with Test dataset\ncombine_dataset = tf.data.Dataset.zip((test_data,tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(res))))\n\n# Convert to pandas for \ndf = pd.DataFrame()\n\nfor element in combine_dataset:\n    test_species = penguin_classes[tf.argmax(element[0][1][0])]\n    predicted_species = penguin_classes[tf.argmax(element[1])]\n\n    same_result = (test_species == predicted_species)\n    row = {'test_species':test_species,'test_res': tf.argmax(element[0][1][0]).numpy(),\n           'predicted_species': predicted_species, 'predicted_res': tf.argmax(element[1]).numpy(),'same_result': same_result   }\n    df = pd.concat([df, pd.DataFrame([row])])\n\n\ndf\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e5ac912-23df-4a13-aeb2-81ad02470817",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Tensorflow doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n\n# Plot the confusion matrix\n\ncm = confusion_matrix(df['test_res'].to_numpy(), df['predicted_res'].to_numpy())\n\nplt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.colorbar()\ntick_marks = np.arange(len(penguin_classes))\nplt.xticks(tick_marks, penguin_classes, rotation=85)\nplt.yticks(tick_marks, penguin_classes)\nplt.xlabel(\"Predicted Species\")\nplt.ylabel(\"Actual Species\")\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe24bd4c-ae05-44f4-b159-2c31495ad372",
   "metadata": {
    "language": "python",
    "name": "get_model_registry",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\nreg = Registry(session=session, database_name=\"TENSORFLOW_DATA\", schema_name=\"FEATURE_STORE\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6af93bc7-0b2b-465c-a1a8-76f6fb812761",
   "metadata": {
    "language": "python",
    "name": "log_model_to_registry",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\ninput = {'features':[[1.,2.,3.,4.]]}\noutput = {'label':[[1,0,0]]}\n\ninput_df = pd.DataFrame(input)\noutput_df = pd.DataFrame(output)\n\nfrom snowflake.ml.model import model_signature\n\npredict_signature = model_signature.infer_signature(input_data=input_df, output_data=output_df)\n\nmv = reg.log_model(\n    trained_model, \n    model_name ='tf_pengiun_model', \n    #version_name='v2', # Auto generate version name\n    pip_requirements=[\"tensorflow[and-cuda]==2.14\"],\n    signatures={\"predict\": predict_signature},\n    options={'relax_version': False},\n    #sample_input_data= df,\n    target_platforms=['SNOWPARK_CONTAINER_SERVICES']\n)\n ",
   "execution_count": null
  }
 ]
}